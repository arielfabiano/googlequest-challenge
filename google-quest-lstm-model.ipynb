{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook has been forked from: https://www.kaggle.com/arvissu/simple-pytorch-lstm/comments?scriptVersionId=25732138 and only modified to respect the experimentation repeteability and metrics obtention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n",
    "                  'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n",
    "                  'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n",
    "                  'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n",
    "                  'question_type_compare', 'question_type_consequence', 'question_type_definition',\n",
    "                  'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n",
    "                  'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n",
    "                  'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
    "                  'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n",
    "                  'answer_type_reason_explanation', 'answer_well_written']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "SEED = 21937\n",
    "set_seeds(SEED)\n",
    "\n",
    "SVD_QTY = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from scipy.stats import spearmanr\n",
    "from gensim.models import Word2Vec\n",
    "from flashtext import KeywordProcessor\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\n",
    "sub = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = pickle.load(open('/kaggle/input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTS = {\n",
    "            '》', '〞', '¢', '‹', '╦', '║', '♪', 'Ø', '╩', '\\\\', '★', '＋', 'ï', '<', '?', '％', '+', '„', 'α', '*', '〰', '｟', '¹', '●', '〗', ']', '▾', '■', '〙', '↓', '´', '【', 'ᴵ',\n",
    "            '\"', '）', '｀', '│', '¤', '²', '‡', '¿', '–', '」', '╔', '〾', '%', '¾', '←', '〔', '＿', '’', '-', ':', '‧', '｛', 'β', '（', '─', 'à', 'â', '､', '•', '；', '☆', '／', 'π',\n",
    "            'é', '╗', '＾', '▪', ',', '►', '/', '〚', '¶', '♦', '™', '}', '″', '＂', '『', '▬', '±', '«', '“', '÷', '×', '^', '!', '╣', '▲', '・', '░', '′', '〝', '‛', '√', ';', '】', '▼',\n",
    "            '.', '~', '`', '。', 'ə', '］', '，', '{', '～', '！', '†', '‘', '﹏', '═', '｣', '〕', '〜', '＼', '▒', '＄', '♥', '〛', '≤', '∞', '_', '[', '＆', '→', '»', '－', '＝', '§', '⋅', \n",
    "            '▓', '&', 'Â', '＞', '〃', '|', '¦', '—', '╚', '〖', '―', '¸', '³', '®', '｠', '¨', '‟', '＊', '£', '#', 'Ã', \"'\", '▀', '·', '？', '、', '█', '”', '＃', '⊕', '=', '〟', '½', '』',\n",
    "            '［', '$', ')', 'θ', '@', '›', '＠', '｝', '¬', '…', '¼', '：', '¥', '❤', '€', '−', '＜', '(', '〘', '▄', '＇', '>', '₤', '₹', '∅', 'è', '〿', '「', '©', '｢', '∙', '°', '｜', '¡', \n",
    "            '↑', 'º', '¯', '♫', '#'\n",
    "          }\n",
    "\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\",\n",
    "\"couldnt\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\",\n",
    "\"doesnt\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\", \"havent\" : \"have not\", \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\", \"isn't\" : \"is not\", \"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \n",
    "\"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"shouldnt\" : \"should not\",\n",
    "\"that's\" : \"that is\", \"thats\" : \"that is\", \"there's\" : \"there is\", \"theres\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\", \"theyre\":  \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\", \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\", \"tryin'\":\"trying\"}\n",
    "\n",
    "\n",
    "def clean_punct(text):\n",
    "  text = str(text)\n",
    "  for punct in PUNCTS:\n",
    "    text = text.replace(punct, ' {} '.format(punct))\n",
    "  \n",
    "  return text\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = KeywordProcessor(case_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in mispell_dict.items():\n",
    "    kp.add_keyword(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = kp.replace_keywords(text)\n",
    "    text = clean_punct(text)\n",
    "    text = re.sub(r'\\n\\r', ' ', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    \n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_title'] = train['question_title'].apply(lambda x : preprocessing(x))\n",
    "train['clean_body'] = train['question_body'].apply(lambda x : preprocessing(x))\n",
    "train['clean_answer'] = train['answer'].apply(lambda x : preprocessing(x))\n",
    "\n",
    "test['clean_title'] = test['question_title'].apply(lambda x : preprocessing(x))\n",
    "test['clean_body'] = test['question_body'].apply(lambda x : preprocessing(x))\n",
    "test['clean_answer'] = test['answer'].apply(lambda x : preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns = ['question_asker_intent_understanding',\n",
    "       'question_body_critical', 'question_conversational',\n",
    "       'question_expect_short_answer', 'question_fact_seeking',\n",
    "       'question_has_commonly_accepted_answer',\n",
    "       'question_interestingness_others', 'question_interestingness_self',\n",
    "       'question_multi_intent', 'question_not_really_a_question',\n",
    "       'question_opinion_seeking', 'question_type_choice',\n",
    "       'question_type_compare', 'question_type_consequence',\n",
    "       'question_type_definition', 'question_type_entity',\n",
    "       'question_type_instructions', 'question_type_procedure',\n",
    "       'question_type_reason_explanation', 'question_type_spelling',\n",
    "       'question_well_written', 'answer_helpful',\n",
    "       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
    "       'answer_satisfaction', 'answer_type_instructions',\n",
    "       'answer_type_procedure', 'answer_type_reason_explanation',\n",
    "       'answer_well_written']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(word_index):\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    \n",
    "    unknown_words = []\n",
    "    unk = {}\n",
    "    known_count = 0\n",
    "    unk_count = 0\n",
    "    for word, i in word_index.items():\n",
    "        if word in MODEL:\n",
    "            embedding_matrix[i] = MODEL[word]\n",
    "            known_count += 1\n",
    "            continue\n",
    "        if word.lower() in MODEL:\n",
    "            embedding_matrix[i] = MODEL[word.lower()]\n",
    "            known_count += 1\n",
    "            continue    \n",
    "        if word.upper() in MODEL:\n",
    "            embedding_matrix[i] = MODEL[word.upper()]\n",
    "            known_count += 1\n",
    "            continue\n",
    "        if word.capitalize() in MODEL:\n",
    "            embedding_matrix[i] = MODEL[word.capitalize()]\n",
    "            known_count += 1\n",
    "            continue\n",
    "        if unidecode.unidecode(word) in MODEL:\n",
    "            embedding_matrix[i] = MODEL[unidecode.unidecode(word)]\n",
    "            known_count += 1\n",
    "            continue\n",
    "        try:\n",
    "            unk[word] += 1 \n",
    "        except:\n",
    "            unk[word] = 1\n",
    "        \n",
    "        unk_count += 1\n",
    "    \n",
    "    \n",
    "    print('all token in embedding percentage : {:.2f}%'.format( known_count/(unk_count+known_count)  * 100))\n",
    "#     print('token in embedding percentage : {}'.format( known_count/(unk_count+known_count)))\n",
    "    return embedding_matrix, unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(filters='', lower=False)\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(list(train['clean_title']) + list(train['clean_body']) + list(train['clean_answer']) \\\n",
    "                        + list(test['clean_title']) + list(test['clean_body']) + list(test['clean_answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_MAX_LEN = 50\n",
    "BODY_MAX_LEN = 500\n",
    "ANSWER_MAX_LEN = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_title_len'] = train['clean_title'].apply(lambda x : len(x))\n",
    "train['clean_body_len'] = train['clean_body'].apply(lambda x : len(x))\n",
    "train['clean_answer_len'] = train['clean_answer'].apply(lambda x : len(x))\n",
    "\n",
    "\n",
    "test['clean_title_len'] = test['clean_title'].apply(lambda x : len(x))\n",
    "test['clean_body_len'] = test['clean_body'].apply(lambda x : len(x))\n",
    "test['clean_answer_len'] = test['clean_answer'].apply(lambda x : len(x))\n",
    "\n",
    "# train title max 58 test 48\n",
    "\n",
    "# train body max 4924 test body max 1894\n",
    "\n",
    "# train answer max 8194 test max 2224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_title = tokenizer.texts_to_sequences(train['clean_title'])\n",
    "x_test_title = tokenizer.texts_to_sequences(test['clean_title'])\n",
    "\n",
    "x_train_body = tokenizer.texts_to_sequences(train['clean_body'])\n",
    "x_test_body = tokenizer.texts_to_sequences(test['clean_body'])\n",
    "\n",
    "x_train_answer = tokenizer.texts_to_sequences(train['clean_answer'])\n",
    "x_test_answer = tokenizer.texts_to_sequences(test['clean_answer'])\n",
    "\n",
    "\n",
    "x_train_title = sequence.pad_sequences(x_train_title, maxlen=TITLE_MAX_LEN,padding='post')\n",
    "x_test_title = sequence.pad_sequences(x_test_title, maxlen=TITLE_MAX_LEN,padding='post')\n",
    "\n",
    "x_train_body = sequence.pad_sequences(x_train_body, maxlen=BODY_MAX_LEN,padding='post')\n",
    "x_test_body = sequence.pad_sequences(x_test_body, maxlen=BODY_MAX_LEN,padding='post')\n",
    "\n",
    "x_train_answer = sequence.pad_sequences(x_train_answer, maxlen=ANSWER_MAX_LEN,padding='post')\n",
    "x_test_answer = sequence.pad_sequences(x_test_answer, maxlen=ANSWER_MAX_LEN,padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove origin column : host\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "c = 'host'\n",
    "onehotencoder = OneHotEncoder(sparse=False, categories='auto').fit(np.concatenate((train[c].values.reshape(-1, 1).astype('str'), test[c].values.reshape(-1, 1).astype('str'))))\n",
    "train_trans = onehotencoder.transform(train[c].values.reshape(-1, 1).astype('str'))\n",
    "test_trans = onehotencoder.transform(test[c].values.reshape(-1, 1).astype('str'))\n",
    "for i in range(train_trans.shape[1]):\n",
    "    train['{}_{}'.format(c, i)] = train_trans[:, i]\n",
    "    test['{}_{}'.format(c, i)] = test_trans[:, i]\n",
    "print('remove origin column : {}'.format(c))\n",
    "train = train.drop(columns=c)\n",
    "test = test.drop(columns=c)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_category = pd.get_dummies(train['category'].values).values\n",
    "test_category = pd.get_dummies(test['category'].values).values\n",
    "\n",
    "hosts = ['host_{}'.format(i) for i in range(64)]\n",
    "train_host = train.loc[:, hosts].values\n",
    "test_host = test.loc[:, hosts].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all token in embedding percentage : 79.00%\n"
     ]
    }
   ],
   "source": [
    "word2vec_matrix, unk = build_matrix(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "\n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "\n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "class SpatialDropout(nn.Module):\n",
    "    def __init__(self,p):\n",
    "        super(SpatialDropout, self).__init__()\n",
    "        self.dropout = nn.Dropout2d(p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)   # convert to [batch, feature, timestep]\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1)   # back to [batch, timestep, feature]\n",
    "        return x\n",
    "\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_unit, num_layer=1):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.max_feature = embedding_matrix.shape[0]\n",
    "        self.embedding_size = embedding_matrix.shape[1]\n",
    "      \n",
    "        self.embedding_body = nn.Embedding(self.max_feature, self.embedding_size)\n",
    "        self.embedding_body.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding_body.weight.required_grad = False\n",
    "        \n",
    "        self.embedding_answer = nn.Embedding(self.max_feature, self.embedding_size)\n",
    "        self.embedding_answer.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding_answer.weight.required_grad = False\n",
    "        \n",
    "        self.embedding_title = nn.Embedding(self.max_feature, self.embedding_size)\n",
    "        self.embedding_title.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding_title.weight.required_grad = False\n",
    "        \n",
    "        self.embedding_dropout = SpatialDropout(0.4)\n",
    "        \n",
    "        self.lstm1_body = nn.LSTM(self.embedding_size, hidden_unit, num_layers=num_layer, bidirectional=True, batch_first=True)\n",
    "        self.lstm2_body = nn.LSTM(hidden_unit*2, int(hidden_unit/2), num_layers=num_layer, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm1_answer = nn.LSTM(self.embedding_size, hidden_unit, num_layers=num_layer, bidirectional=True, batch_first=True)\n",
    "        self.lstm2_answer = nn.LSTM(hidden_unit*2, int(hidden_unit/2), num_layers=num_layer, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm1_title = nn.LSTM(self.embedding_size, hidden_unit, num_layers=num_layer, bidirectional=True, batch_first=True)\n",
    "        self.lstm2_title = nn.LSTM(hidden_unit*2, int(hidden_unit/2), num_layers=num_layer, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.attention_body = Attention(hidden_unit, BODY_MAX_LEN)\n",
    "        self.attention_answer = Attention(hidden_unit, ANSWER_MAX_LEN)\n",
    "        self.attention_title = Attention(hidden_unit, TITLE_MAX_LEN)\n",
    "        \n",
    "#         self.category = nn.Embedding(5, 10)\n",
    "#         self.host = nn.Embedding(64, 128)\n",
    "        \n",
    "        self.linear_title = nn.Linear(hidden_unit*3, hidden_unit)\n",
    "        self.linear_body = nn.Linear(hidden_unit*3, hidden_unit)\n",
    "        self.linear_answer = nn.Linear(hidden_unit*3, hidden_unit)\n",
    "        \n",
    "#         self.linear_out = nn.Linear(hidden_unit, 30)\n",
    "        self.additional_category = nn.Linear(5, 5)\n",
    "        self.additional_host = nn.Linear(64, 32)\n",
    "        \n",
    "        self.linear_q = nn.Linear(hidden_unit*2+37, hidden_unit)\n",
    "        self.linear_a = nn.Linear(hidden_unit+37, hidden_unit)\n",
    "        self.linear_q_out = nn.Linear(hidden_unit, 21)\n",
    "        self.linear_a_out = nn.Linear(hidden_unit, 9)\n",
    "        \n",
    "    def forward(self, body, answer, title, category, host):\n",
    "        \n",
    "        x_body = self.embedding_dropout(self.embedding_body(body))\n",
    "        h_lstm1_body, _ = self.lstm1_body(x_body)\n",
    "        h_lstm2_body, _ = self.lstm2_body(h_lstm1_body)\n",
    "        \n",
    "        x_answer = self.embedding_dropout(self.embedding_answer(answer))\n",
    "        h_lstm1_answer, _ = self.lstm1_answer(x_answer)\n",
    "        h_lstm2_answer, _ = self.lstm2_answer(h_lstm1_answer)\n",
    "        \n",
    "        x_title = self.embedding_dropout(self.embedding_title(title))\n",
    "        h_lstm1_title, _ = self.lstm1_title(x_title)\n",
    "        h_lstm2_title, _ = self.lstm2_title(h_lstm1_title)\n",
    "        \n",
    "#         print(h_lstm2_body.size())\n",
    "        att_body = self.attention_body(h_lstm2_body)\n",
    "        att_answer = self.attention_answer(h_lstm2_answer)\n",
    "        att_title = self.attention_title(h_lstm2_title)\n",
    "        \n",
    "        avg_pool_body = torch.mean(h_lstm2_body, 1)\n",
    "        max_pool_body, _ = torch.max(h_lstm2_body, 1)\n",
    "        \n",
    "        avg_pool_answer = torch.mean(h_lstm2_answer, 1)\n",
    "        max_pool_answer, _ = torch.max(h_lstm2_answer, 1)\n",
    "        \n",
    "        avg_pool_title = torch.mean(h_lstm2_title, 1)\n",
    "        max_pool_title, _ = torch.max(h_lstm2_title, 1)\n",
    "        \n",
    "        body_cat = torch.cat((att_body, avg_pool_body, max_pool_body), 1)\n",
    "        answer_cat = torch.cat((att_answer, avg_pool_answer, max_pool_answer), 1)\n",
    "        title_cat = torch.cat((att_title, avg_pool_title, max_pool_title), 1)\n",
    "        \n",
    "        body_cat = torch.relu(self.linear_body(body_cat))\n",
    "        answer_cat = torch.relu(self.linear_answer(answer_cat))\n",
    "        title_cat = torch.relu(self.linear_title(title_cat))\n",
    "\n",
    "        category = self.additional_category(category)\n",
    "        host = self.additional_host(host)\n",
    "        \n",
    "        hidden_q = self.linear_q(torch.cat((title_cat, body_cat, category, host), 1))\n",
    "        hidden_a = self.linear_a(torch.cat((answer_cat, category, host), 1))\n",
    "                                          \n",
    "        q_result = self.linear_q_out(hidden_q)\n",
    "        a_result = self.linear_a_out(hidden_a)\n",
    "        \n",
    "        out = torch.cat([q_result, a_result], 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2020\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LR = 0.001\n",
    "hidden_unit = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLDS = 5\n",
    "scores = []\n",
    "\n",
    "kf = GroupKFold(n_splits=NFOLDS).split(X=train.question_body, groups=train.question_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.TensorDataset):\n",
    "\n",
    "    def __init__(self, body_data, answer_data, title_data, category_data, host_data, idxs, targets=None):\n",
    "        self.body_data = body_data[idxs]\n",
    "        self.answer_data = answer_data[idxs]\n",
    "        self.title_data = title_data[idxs]\n",
    "        self.category_data = category_data[idxs]\n",
    "        self.host_data = host_data[idxs]\n",
    "        self.targets = targets[idxs] if targets is not None else np.zeros((self.body_data.shape[0], 30))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        body = self.body_data[idx]\n",
    "        answer = self.answer_data[idx]\n",
    "        title = self.title_data[idx]\n",
    "        category = self.category_data[idx]\n",
    "        host = self.host_data[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        return body, answer, title, category, host, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.body_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(TextDataset(x_test_body, x_test_answer, x_test_title, test_category, test_host, test.index),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearmanr(preds, trues):\n",
    "    rhos = []\n",
    "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
    "        rhos.append(\n",
    "            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n",
    "    return np.nanmean(rhos), rhos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "epoch = 0, train_loss = 0.4190459871994475, valid_spearman = 0.30170456079546326\n",
      "epoch = 1, train_loss = 0.39031264487073447, valid_spearman = 0.33223485107031686\n",
      "epoch = 2, train_loss = 0.3764026749650016, valid_spearman = 0.3399905226473844\n",
      "epoch = 3, train_loss = 0.3663959198019981, valid_spearman = 0.34428076611016484\n",
      "epoch = 4, train_loss = 0.3554971591998881, valid_spearman = 0.3453158287716578\n",
      "question_asker_intent_understanding rho: 0.3705362251570903\n",
      "question_body_critical rho: 0.6123019482725857\n",
      "question_conversational rho: 0.360342248749893\n",
      "question_expect_short_answer rho: 0.20620144099896348\n",
      "question_fact_seeking rho: 0.2724405948190352\n",
      "question_has_commonly_accepted_answer rho: 0.3875316344045226\n",
      "question_interestingness_others rho: 0.34136213771745993\n",
      "question_interestingness_self rho: 0.47381645963958324\n",
      "question_multi_intent rho: 0.5125014767585364\n",
      "question_not_really_a_question rho: 0.10107538074578039\n",
      "question_opinion_seeking rho: 0.346494672033686\n",
      "question_type_choice rho: 0.6237174897287211\n",
      "question_type_compare rho: 0.3092535298407712\n",
      "question_type_consequence rho: 0.10507033153525372\n",
      "question_type_definition rho: 0.3481576256290016\n",
      "question_type_entity rho: 0.39186432649266784\n",
      "question_type_instructions rho: 0.7233396595775021\n",
      "question_type_procedure rho: 0.3076919469085447\n",
      "question_type_reason_explanation rho: 0.4782993746099758\n",
      "question_type_spelling rho: 0.046625629112845735\n",
      "question_well_written rho: 0.4569592048277025\n",
      "answer_helpful rho: 0.24214583268942588\n",
      "answer_level_of_information rho: 0.35407916768051156\n",
      "answer_plausible rho: 0.1050066662670317\n",
      "answer_relevance rho: 0.09768887316563596\n",
      "answer_satisfaction rho: 0.21043979338186838\n",
      "answer_type_instructions rho: 0.6960633498143981\n",
      "answer_type_procedure rho: 0.19149186547711355\n",
      "answer_type_reason_explanation rho: 0.5591805437744531\n",
      "answer_well_written rho: 0.12779543333917293\n",
      "fold 2\n",
      "epoch = 0, train_loss = 0.4207037934083581, valid_spearman = 0.2775954698675305\n",
      "epoch = 1, train_loss = 0.39074592332071684, valid_spearman = 0.30669547186459734\n",
      "epoch = 2, train_loss = 0.3769610630669321, valid_spearman = 0.3260162628833528\n",
      "epoch = 3, train_loss = 0.3657794976800222, valid_spearman = 0.33819076138351184\n",
      "epoch = 4, train_loss = 0.3557668651816238, valid_spearman = 0.3388429363038361\n",
      "question_asker_intent_understanding rho: 0.3348133509428635\n",
      "question_body_critical rho: 0.6740793170834223\n",
      "question_conversational rho: 0.37737028898339126\n",
      "question_expect_short_answer rho: 0.24729891610770263\n",
      "question_fact_seeking rho: 0.2974261815702689\n",
      "question_has_commonly_accepted_answer rho: 0.362077890697139\n",
      "question_interestingness_others rho: 0.3281788902632006\n",
      "question_interestingness_self rho: 0.4681092557785547\n",
      "question_multi_intent rho: 0.5238723874729784\n",
      "question_not_really_a_question rho: 0.08183618939223442\n",
      "question_opinion_seeking rho: 0.3696366711946303\n",
      "question_type_choice rho: 0.5941819295741733\n",
      "question_type_compare rho: 0.29140384079213805\n",
      "question_type_consequence rho: 0.09618994982537778\n",
      "question_type_definition rho: 0.2656206074035209\n",
      "question_type_entity rho: 0.30251206314513807\n",
      "question_type_instructions rho: 0.6607770540245196\n",
      "question_type_procedure rho: 0.21945100619183458\n",
      "question_type_reason_explanation rho: 0.4317058505943455\n",
      "question_type_spelling rho: 0.06868303028516964\n",
      "question_well_written rho: 0.48826833578773976\n",
      "answer_helpful rho: 0.19158506191548874\n",
      "answer_level_of_information rho: 0.37912421737955543\n",
      "answer_plausible rho: 0.15240539576158676\n",
      "answer_relevance rho: 0.1612574633445834\n",
      "answer_satisfaction rho: 0.2717359341114438\n",
      "answer_type_instructions rho: 0.6627036082351818\n",
      "answer_type_procedure rho: 0.20032944636948163\n",
      "answer_type_reason_explanation rho: 0.5247577677724793\n",
      "answer_well_written rho: 0.13789618711493878\n",
      "fold 3\n",
      "epoch = 0, train_loss = 0.42054819851386166, valid_spearman = 0.29725430639485423\n",
      "epoch = 1, train_loss = 0.39105802070927503, valid_spearman = 0.33594485430840615\n",
      "epoch = 2, train_loss = 0.37853193846534494, valid_spearman = 0.35451547593189725\n",
      "epoch = 3, train_loss = 0.3671066553496695, valid_spearman = 0.3528110848789962\n",
      "epoch = 4, train_loss = 0.357419110881467, valid_spearman = 0.3558883401839081\n",
      "question_asker_intent_understanding rho: 0.3762097271629202\n",
      "question_body_critical rho: 0.6237585908824185\n",
      "question_conversational rho: 0.42641969763661175\n",
      "question_expect_short_answer rho: 0.23536890365590954\n",
      "question_fact_seeking rho: 0.2895550434263905\n",
      "question_has_commonly_accepted_answer rho: 0.43767718467284844\n",
      "question_interestingness_others rho: 0.34260397411816684\n",
      "question_interestingness_self rho: 0.5005434105708414\n",
      "question_multi_intent rho: 0.5221038436428427\n",
      "question_not_really_a_question rho: 0.09462585128220392\n",
      "question_opinion_seeking rho: 0.4002727262259755\n",
      "question_type_choice rho: 0.639242371841954\n",
      "question_type_compare rho: 0.31914808702900876\n",
      "question_type_consequence rho: 0.1054305751315328\n",
      "question_type_definition rho: 0.3200645602560773\n",
      "question_type_entity rho: 0.3123425027555301\n",
      "question_type_instructions rho: 0.7179758852725998\n",
      "question_type_procedure rho: 0.2601241844146052\n",
      "question_type_reason_explanation rho: 0.4599059736751766\n",
      "question_type_spelling rho: 0.06024216519866703\n",
      "question_well_written rho: 0.5220228186732774\n",
      "answer_helpful rho: 0.21064461843649185\n",
      "answer_level_of_information rho: 0.41705308497570664\n",
      "answer_plausible rho: 0.12345242981800966\n",
      "answer_relevance rho: 0.1283388883600484\n",
      "answer_satisfaction rho: 0.2424533284492575\n",
      "answer_type_instructions rho: 0.7092163812250107\n",
      "answer_type_procedure rho: 0.20301093477495116\n",
      "answer_type_reason_explanation rho: 0.5519840698030345\n",
      "answer_well_written rho: 0.12485839214917353\n",
      "fold 4\n",
      "epoch = 0, train_loss = 0.4210399175316789, valid_spearman = 0.2872154760818947\n",
      "epoch = 1, train_loss = 0.3923039680776686, valid_spearman = 0.3183651935083089\n",
      "epoch = 2, train_loss = 0.37894197023627957, valid_spearman = 0.3396334475303521\n",
      "epoch = 3, train_loss = 0.3682977687459246, valid_spearman = 0.340724177416473\n",
      "epoch = 4, train_loss = 0.3579828073050718, valid_spearman = 0.33714977597316487\n",
      "question_asker_intent_understanding rho: 0.29999192401263813\n",
      "question_body_critical rho: 0.6052561552441911\n",
      "question_conversational rho: 0.3826089391214874\n",
      "question_expect_short_answer rho: 0.22933877646196388\n",
      "question_fact_seeking rho: 0.2590677308473489\n",
      "question_has_commonly_accepted_answer rho: 0.3477770279771959\n",
      "question_interestingness_others rho: 0.3709853417080705\n",
      "question_interestingness_self rho: 0.4777213289470494\n",
      "question_multi_intent rho: 0.5196114632632436\n",
      "question_not_really_a_question rho: 0.04759445928840904\n",
      "question_opinion_seeking rho: 0.3494214328685251\n",
      "question_type_choice rho: 0.6643941786776569\n",
      "question_type_compare rho: 0.2864667842496226\n",
      "question_type_consequence rho: 0.11005641447640102\n",
      "question_type_definition rho: 0.3274623433256289\n",
      "question_type_entity rho: 0.3030856773316347\n",
      "question_type_instructions rho: 0.6960490712596318\n",
      "question_type_procedure rho: 0.27298326212767765\n",
      "question_type_reason_explanation rho: 0.5155623712701172\n",
      "question_type_spelling rho: 0.05769828307457758\n",
      "question_well_written rho: 0.43754586343568186\n",
      "answer_helpful rho: 0.15836513400818533\n",
      "answer_level_of_information rho: 0.38842198216032264\n",
      "answer_plausible rho: 0.11293645692193056\n",
      "answer_relevance rho: 0.11635036500417889\n",
      "answer_satisfaction rho: 0.21774787850386615\n",
      "answer_type_instructions rho: 0.7015091125736632\n",
      "answer_type_procedure rho: 0.1767204852357472\n",
      "answer_type_reason_explanation rho: 0.5729088762932832\n",
      "answer_well_written rho: 0.10885415952501568\n",
      "fold 5\n",
      "epoch = 0, train_loss = 0.4200520511232105, valid_spearman = 0.2789468143413266\n",
      "epoch = 1, train_loss = 0.39033828246018487, valid_spearman = 0.3133275222556897\n",
      "epoch = 2, train_loss = 0.3771225763772544, valid_spearman = 0.33182130921576103\n",
      "epoch = 3, train_loss = 0.36614572101910275, valid_spearman = 0.33718631361254225\n",
      "epoch = 4, train_loss = 0.3563141057802333, valid_spearman = 0.3331378849423218\n",
      "question_asker_intent_understanding rho: 0.3255792750730929\n",
      "question_body_critical rho: 0.58420573789786\n",
      "question_conversational rho: 0.3240421868637245\n",
      "question_expect_short_answer rho: 0.2424387398951495\n",
      "question_fact_seeking rho: 0.2298602405990191\n",
      "question_has_commonly_accepted_answer rho: 0.3453987067676499\n",
      "question_interestingness_others rho: 0.3116802275069976\n",
      "question_interestingness_self rho: 0.4755664926148351\n",
      "question_multi_intent rho: 0.5091730764896788\n",
      "question_not_really_a_question rho: 0.08275564772869541\n",
      "question_opinion_seeking rho: 0.28668105218021644\n",
      "question_type_choice rho: 0.6000398953099796\n",
      "question_type_compare rho: 0.25769356374503594\n",
      "question_type_consequence rho: 0.132915589710987\n",
      "question_type_definition rho: 0.3393783412181114\n",
      "question_type_entity rho: 0.35481788873262565\n",
      "question_type_instructions rho: 0.6755551897427853\n",
      "question_type_procedure rho: 0.23040066599982925\n",
      "question_type_reason_explanation rho: 0.504541453416964\n",
      "question_type_spelling rho: 0.0858659533044188\n",
      "question_well_written rho: 0.4945822097247535\n",
      "answer_helpful rho: 0.17897798501958065\n",
      "answer_level_of_information rho: 0.3817305734000151\n",
      "answer_plausible rho: 0.07700741693320957\n",
      "answer_relevance rho: 0.12385298822841702\n",
      "answer_satisfaction rho: 0.2206709755995233\n",
      "answer_type_instructions rho: 0.6849530361959943\n",
      "answer_type_procedure rho: 0.2146955857735668\n",
      "answer_type_reason_explanation rho: 0.5845240891194301\n",
      "answer_well_written rho: 0.13455176347750833\n"
     ]
    }
   ],
   "source": [
    "y = train.loc[:, y_columns].values\n",
    "\n",
    "oof = np.zeros((len(train), 30))\n",
    "test_pred = np.zeros((len(test), 30))\n",
    "\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(kf):\n",
    "    print(f'fold {i+1}')\n",
    "    gc.collect()\n",
    "    train_loader = torch.utils.data.DataLoader(TextDataset(x_train_body, x_train_answer, x_train_title, train_category, train_host, train_idx, y),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(TextDataset(x_train_body, x_train_answer, x_train_title, train_category, train_host, valid_idx, y),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    net = LSTM_Model(word2vec_matrix, hidden_unit)\n",
    "    net.cuda()\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean').cuda()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "    for epoch in range(EPOCHS):  \n",
    "        start_time = time.time()\n",
    "        avg_loss = 0.0\n",
    "        net.train()\n",
    "        for data in train_loader:\n",
    "\n",
    "            # get the inputs\n",
    "            body, answer, title, category, host, labels = data\n",
    "            pred = net(body.long().cuda(), answer.long().cuda(), title.long().cuda(), category.float().cuda(), host.float().cuda())\n",
    "\n",
    "            loss = loss_fn(pred, labels.cuda())\n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the Tensors it will update (which are the learnable weights\n",
    "            # of the model)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # Calling the step function on an Optimizer makes an update to its parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = 0.0\n",
    "        net.eval()\n",
    "\n",
    "        valid_preds = np.zeros((len(valid_idx), 30))\n",
    "        true_label = np.zeros((len(valid_idx), 30))\n",
    "        for j, data in enumerate(val_loader):\n",
    "\n",
    "            # get the inputs\n",
    "            body, answer, title, category, host, labels = data\n",
    "\n",
    "            ## forward + backward + optimize\n",
    "            pred = net(body.long().cuda(), answer.long().cuda(), title.long().cuda(), category.float().cuda(), host.float().cuda())\n",
    "\n",
    "            loss_val = loss_fn(pred, labels.cuda())\n",
    "            avg_val_loss += loss_val.item()\n",
    "\n",
    "            valid_preds[j * BATCH_SIZE:(j+1) * BATCH_SIZE] = torch.sigmoid(pred).cpu().detach().numpy()\n",
    "            true_label[j * BATCH_SIZE:(j+1) * BATCH_SIZE]  = labels\n",
    "            \n",
    "        score = 0\n",
    "        \n",
    "        score, rho_cols = compute_spearmanr(valid_preds, true_label)\n",
    "             \n",
    "        oof[valid_idx] = valid_preds\n",
    "        elapsed_time = time.time() - start_time \n",
    "        current_loss = avg_loss / len(train_loader)\n",
    "        print(f'epoch = {epoch}, train_loss = {current_loss}, valid_spearman = {score}')\n",
    "    \n",
    "    rho_print = [print(target_columns[i] + \" rho: \" + str(rho_cols[i]) ) for i in range(0, len(target_columns))]      \n",
    "        \n",
    "        \n",
    "    test_pred_fold = np.zeros((len(test), 30))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for q, data in enumerate(test_loader):\n",
    "            body, answer, title, category, host, _ = data\n",
    "            y_pred = net(body.long().cuda(), answer.long().cuda(), title.long().cuda(), category.float().cuda(), host.float().cuda())\n",
    "            test_pred_fold[q * BATCH_SIZE:(q+1) * BATCH_SIZE] = torch.sigmoid(y_pred).cpu().detach().numpy()\n",
    "    test_pred += test_pred_fold/NFOLDS        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.loc[:, y_columns] = test_pred\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.936144</td>\n",
       "      <td>0.640050</td>\n",
       "      <td>0.032274</td>\n",
       "      <td>0.651852</td>\n",
       "      <td>0.847219</td>\n",
       "      <td>0.857711</td>\n",
       "      <td>0.652525</td>\n",
       "      <td>0.543561</td>\n",
       "      <td>0.677015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.908978</td>\n",
       "      <td>0.946579</td>\n",
       "      <td>0.598850</td>\n",
       "      <td>0.976081</td>\n",
       "      <td>0.978747</td>\n",
       "      <td>0.869037</td>\n",
       "      <td>0.034418</td>\n",
       "      <td>0.040883</td>\n",
       "      <td>0.899706</td>\n",
       "      <td>0.928820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.856562</td>\n",
       "      <td>0.458335</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.704505</td>\n",
       "      <td>0.707423</td>\n",
       "      <td>0.859508</td>\n",
       "      <td>0.549353</td>\n",
       "      <td>0.433294</td>\n",
       "      <td>0.036427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638895</td>\n",
       "      <td>0.936889</td>\n",
       "      <td>0.630204</td>\n",
       "      <td>0.964261</td>\n",
       "      <td>0.974905</td>\n",
       "      <td>0.853743</td>\n",
       "      <td>0.935079</td>\n",
       "      <td>0.121270</td>\n",
       "      <td>0.077622</td>\n",
       "      <td>0.882378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.908325</td>\n",
       "      <td>0.630798</td>\n",
       "      <td>0.031167</td>\n",
       "      <td>0.778423</td>\n",
       "      <td>0.886075</td>\n",
       "      <td>0.888329</td>\n",
       "      <td>0.611545</td>\n",
       "      <td>0.504106</td>\n",
       "      <td>0.144779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860368</td>\n",
       "      <td>0.907379</td>\n",
       "      <td>0.579824</td>\n",
       "      <td>0.955478</td>\n",
       "      <td>0.954888</td>\n",
       "      <td>0.819425</td>\n",
       "      <td>0.059973</td>\n",
       "      <td>0.059937</td>\n",
       "      <td>0.708361</td>\n",
       "      <td>0.908481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.887946</td>\n",
       "      <td>0.405688</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.762925</td>\n",
       "      <td>0.763475</td>\n",
       "      <td>0.947691</td>\n",
       "      <td>0.538171</td>\n",
       "      <td>0.423276</td>\n",
       "      <td>0.062677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679997</td>\n",
       "      <td>0.959784</td>\n",
       "      <td>0.704820</td>\n",
       "      <td>0.977221</td>\n",
       "      <td>0.982429</td>\n",
       "      <td>0.897880</td>\n",
       "      <td>0.770389</td>\n",
       "      <td>0.205624</td>\n",
       "      <td>0.561175</td>\n",
       "      <td>0.917200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.924272</td>\n",
       "      <td>0.483797</td>\n",
       "      <td>0.022316</td>\n",
       "      <td>0.830243</td>\n",
       "      <td>0.830808</td>\n",
       "      <td>0.857871</td>\n",
       "      <td>0.642288</td>\n",
       "      <td>0.601753</td>\n",
       "      <td>0.168191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766015</td>\n",
       "      <td>0.884770</td>\n",
       "      <td>0.617468</td>\n",
       "      <td>0.939845</td>\n",
       "      <td>0.938378</td>\n",
       "      <td>0.796577</td>\n",
       "      <td>0.239310</td>\n",
       "      <td>0.147120</td>\n",
       "      <td>0.498535</td>\n",
       "      <td>0.889071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.936144                0.640050   \n",
       "1     46                             0.856562                0.458335   \n",
       "2     70                             0.908325                0.630798   \n",
       "3    132                             0.887946                0.405688   \n",
       "4    200                             0.924272                0.483797   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.032274                      0.651852   \n",
       "1                 0.006017                      0.704505   \n",
       "2                 0.031167                      0.778423   \n",
       "3                 0.001825                      0.762925   \n",
       "4                 0.022316                      0.830243   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.847219                               0.857711   \n",
       "1               0.707423                               0.859508   \n",
       "2               0.886075                               0.888329   \n",
       "3               0.763475                               0.947691   \n",
       "4               0.830808                               0.857871   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.652525                       0.543561   \n",
       "1                         0.549353                       0.433294   \n",
       "2                         0.611545                       0.504106   \n",
       "3                         0.538171                       0.423276   \n",
       "4                         0.642288                       0.601753   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.677015  ...               0.908978        0.946579   \n",
       "1               0.036427  ...               0.638895        0.936889   \n",
       "2               0.144779  ...               0.860368        0.907379   \n",
       "3               0.062677  ...               0.679997        0.959784   \n",
       "4               0.168191  ...               0.766015        0.884770   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.598850          0.976081          0.978747   \n",
       "1                     0.630204          0.964261          0.974905   \n",
       "2                     0.579824          0.955478          0.954888   \n",
       "3                     0.704820          0.977221          0.982429   \n",
       "4                     0.617468          0.939845          0.938378   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.869037                  0.034418               0.040883   \n",
       "1             0.853743                  0.935079               0.121270   \n",
       "2             0.819425                  0.059973               0.059937   \n",
       "3             0.897880                  0.770389               0.205624   \n",
       "4             0.796577                  0.239310               0.147120   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.899706             0.928820  \n",
       "1                        0.077622             0.882378  \n",
       "2                        0.708361             0.908481  \n",
       "3                        0.561175             0.917200  \n",
       "4                        0.498535             0.889071  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
