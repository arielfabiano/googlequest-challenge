{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Google_Quest_PyTorchBERT_finetune_final",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c616e1d5fd7c40389ef58cafc1348622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c9141c1a3dce4800b5ea95d486a74d53",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_aecba41b3f804a6ea7cd5c7eea1cda52",
              "IPY_MODEL_bcddd7e172524e498a70ef8157e69bcc"
            ]
          }
        },
        "c9141c1a3dce4800b5ea95d486a74d53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aecba41b3f804a6ea7cd5c7eea1cda52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2e4d666095514409989e01e507e77179",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efa528b949aa4c328f2b26a25bc721bf"
          }
        },
        "bcddd7e172524e498a70ef8157e69bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4c1f8bfc501d4991a46a190de2121df9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "6079it [00:17, 350.10it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5666c61057cd4cbb990f48361b379d36"
          }
        },
        "2e4d666095514409989e01e507e77179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efa528b949aa4c328f2b26a25bc721bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c1f8bfc501d4991a46a190de2121df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5666c61057cd4cbb990f48361b379d36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0295b92afea341b99684612c1442832d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bc4878fcfeb14746b246b11d734e01c6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f0131405800b48a48429d237802fcf82",
              "IPY_MODEL_a40b2f4b186b47d8986c64c35c2b5d6c"
            ]
          }
        },
        "bc4878fcfeb14746b246b11d734e01c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0131405800b48a48429d237802fcf82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_54da01792b054316b4678b5bdb2c9b70",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a39c5b3a4794dbb99cab7febb317257"
          }
        },
        "a40b2f4b186b47d8986c64c35c2b5d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_81de10cbe01544a285ae86c9a38860e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "6079it [00:17, 349.35it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ee3b5432cddd4d84828c1cfd0b2eed33"
          }
        },
        "54da01792b054316b4678b5bdb2c9b70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a39c5b3a4794dbb99cab7febb317257": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "81de10cbe01544a285ae86c9a38860e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ee3b5432cddd4d84828c1cfd0b2eed33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5939841ce19a4f5cb20bb947e2534ddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_64c63c4620e2480a927a5de4b25beaee",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_856eef928cf34ae2b0034685ef01b857",
              "IPY_MODEL_20e6350cb8224c6dbb43b68ec31a959a"
            ]
          }
        },
        "64c63c4620e2480a927a5de4b25beaee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "856eef928cf34ae2b0034685ef01b857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_50a6dbaef49e463ca91f3e17699674d3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c1e934bad4b749b2b01ea654bfeb94ce"
          }
        },
        "20e6350cb8224c6dbb43b68ec31a959a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_47c835df1bd94342b2845cb805bf6c04",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "6079it [00:30, 200.27it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc8b6db94abe424cb7f309d9aca6f194"
          }
        },
        "50a6dbaef49e463ca91f3e17699674d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c1e934bad4b749b2b01ea654bfeb94ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "47c835df1bd94342b2845cb805bf6c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc8b6db94abe424cb7f309d9aca6f194": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06Qu8Ld78Pq6",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waanjNm8zsPJ",
        "colab_type": "text"
      },
      "source": [
        "### Requirements and setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oU_k5spbCwk",
        "colab_type": "text"
      },
      "source": [
        "##### Set up PyTorch and Apex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fykczxn-ckZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch==1.4.0 &>> tmp.log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WewEi-z5UAHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchvision==0.5.0 &>> tmp.log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pCZKdQ9rVNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile apex_setup.sh\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex  &>> tmp.log\n",
        "cd apex\n",
        "pip install -v --no-cache-dir ./  &>> tmp.log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJ_dcz2rbjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sh apex_setup.sh &>> tmp.log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wTqKIfehbXW3",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "\n",
        "try:\n",
        "    from apex import amp\n",
        "    APEX_AVAILABLE = True\n",
        "    print(\"Apex enabled!\")\n",
        "except ModuleNotFoundError:\n",
        "    APEX_AVAILABLE = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA1PO9Nnaoab",
        "colab_type": "text"
      },
      "source": [
        "##### Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj5rNbWuTfRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install category_encoders &>> tmp.log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25jiDUPyOIjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers &>> tmp.log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKMmw5CAjCSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchcontrib &>> tmp.log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEbivRvAWNqZ",
        "colab_type": "text"
      },
      "source": [
        "### Setting GDrive connection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm-Q_M83WJWy",
        "colab_type": "code",
        "outputId": "849fd195-fe9e-49e9-92c8-8c951885da6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PxxQx2xfQMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/google-quest-challenge/'\n",
        "MODEL_NAME = 'pytorch-bert'\n",
        "sys.path.append(PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbaXIoxmlsq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def set_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "SEED = 21937\n",
        "set_seeds(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Nk77Fw2jyH",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQHkTbeKzeQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_dset = pd.read_csv(PATH+\"data/train.csv\")\n",
        "test_dset = pd.read_csv(PATH+\"data/test.csv\")\n",
        "submi_dset = pd.read_csv(PATH+'data/sample_submission.csv')\n",
        "\n",
        "free_text_columns = ['question_title', 'question_body', 'answer']\n",
        "category_columns = ['host', 'category']\n",
        "discard_columns = ['question_user_name', 'question_user_page',  'answer_user_name', 'answer_user_page', 'url']\n",
        "\n",
        "target_columns = ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n",
        "                  'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n",
        "                  'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n",
        "                  'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n",
        "                  'question_type_compare', 'question_type_consequence', 'question_type_definition',\n",
        "                  'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n",
        "                  'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n",
        "                  'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
        "                  'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n",
        "                  'answer_type_reason_explanation', 'answer_well_written']\n",
        "\n",
        "\n",
        "train_dset = train_dset.drop(discard_columns, axis=1)\n",
        "# (train_dset['question_title'].apply(lambda x: word_tokenize(len(x)))).describe(percentile=[0.75, 0.9, 0.95, 0.95])\n",
        "# [len(np.unique(train_dset[[col]])) for col in target_columns]\n",
        "\n",
        "test_ids = test_dset.index\n",
        "test_dset = test_dset.drop(discard_columns, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MclAZMQYzy8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_run = False\n",
        "if test_run:\n",
        "    train_dset = train_dset[0:20]\n",
        "    test_dset = test_dset[0:20]\n",
        "    submi_dset = submi_dset[0:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRs3_vbpRaWI",
        "colab_type": "text"
      },
      "source": [
        "### Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX2VhXRtRV-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from category_encoders.one_hot import OneHotEncoder\n",
        "from utils.nlp_tools import NlpUtils\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import (sent_tokenize,\n",
        "                  word_tokenize,\n",
        "                  pos_tag)\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "nlp_utils = NlpUtils()\n",
        "STOPWORDS_SET = set(stopwords.words('english'))\n",
        "PUNCTUATION_SET = {';', ':', ',', '.', '!', '?', '\\n', '\\r', '-', '\\(', ')', '`', '$', '<', '>', '=', \n",
        "                   '+', '_', '&', '\\'', '\"', '\\|', '#', '%', '*', '\\[', ']', '\\{', '}'}\n",
        "QUESTION_WORDS = {'who', 'what', 'why', 'how', 'where', 'when', 'with', 'whose', 'whom', 'if', 'or'}\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    return text.apply(lambda x: pd.Series(nlp_utils.nlp_text(x)))\n",
        "\n",
        "\n",
        "def oh_encoder() -> Pipeline:\n",
        "    return Pipeline([('OHE', OneHotEncoder(drop_invariant=True))], verbose=True)\n",
        "\n",
        "\n",
        "def custom_transformer(method) -> Pipeline:\n",
        "    return Pipeline([\n",
        "        ('Custom Function', FunctionTransformer(method, validate=False)),\n",
        "        ], verbose=True)\n",
        "\n",
        "\n",
        "@custom_transformer\n",
        "def char_count(text: pd.Series) -> int:\n",
        "    return pd.DataFrame(text.apply(lambda row: len(row)))\n",
        "\n",
        "\n",
        "@custom_transformer\n",
        "def word_count(text: pd.Series) -> int:\n",
        "    \"\"\" Given a preprocessed text it returns the quantity of words\"\"\"\n",
        "    return pd.DataFrame(text.apply(lambda row: len(word_tokenize(row))))\n",
        "\n",
        "\n",
        "@custom_transformer\n",
        "def unique_word_count(text: pd.Series) -> int:\n",
        "    \"\"\" Given a preprocessed text it returns the quantity of unique words \"\"\"\n",
        "    return pd.DataFrame(text.apply(lambda row: len(set(word_tokenize(row)))))\n",
        "\n",
        "\n",
        "@custom_transformer\n",
        "def sentence_count(text: pd.Series) -> int:\n",
        "    return pd.DataFrame(text.apply(lambda row: len(sent_tokenize(row))))\n",
        "\n",
        "\n",
        "def stopword_ratio_calc(text: str) -> float:\n",
        "    tokenized_text = word_tokenize(text)\n",
        "    word_count = len(tokenized_text)\n",
        "    stopword_count = sum([1 if word.lower() in STOPWORDS_SET else 0 for word in tokenized_text])\n",
        "    try:\n",
        "        return stopword_count/word_count\n",
        "    except ZeroDivisionError:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "@custom_transformer\n",
        "def stopword_ratio(text: pd.Series) -> pd.DataFrame:\n",
        "    return pd.DataFrame(text.apply(lambda row: stopword_ratio_calc(row)))\n",
        "\n",
        "\n",
        "def uppercase_ratio_calc(text: str) -> (float, float):\n",
        "    tokens = word_tokenize(text)\n",
        "    word_qty = len(tokens)\n",
        "    word_uppercase_count = sum([1 if word[0].isupper() else 0 for word in tokens])\n",
        "    char_qty = len(text)\n",
        "    total_uppercase_count = len(re.findall(r'[A-Z]', text))\n",
        "    try:\n",
        "        word_ratio = word_uppercase_count / word_qty\n",
        "        char_ratio = total_uppercase_count / char_qty\n",
        "    except ZeroDivisionError:\n",
        "        word_ratio = char_ratio = 0.0\n",
        "    return word_ratio, char_ratio\n",
        "\n",
        "\n",
        "@custom_transformer\n",
        "def uppercase_ratio(text: pd.Series) -> pd.DataFrame:\n",
        "    \"\"\" Given a word tokenized text it returns the ratio of words that begin with uppercase\n",
        "        and the ratio of uppercase letters\"\"\"\n",
        "    return text.to_frame().apply(lambda row: uppercase_ratio_calc(row[0]), result_type='expand', axis=1)\n",
        "\n",
        "\n",
        "def punctuation_count_calc(text: str) -> pd.DataFrame:\n",
        "    punct_counter = 0\n",
        "    for i, punctuation in enumerate(PUNCTUATION_SET):\n",
        "        punct_counter += len(re.findall('[{}]'.format(punctuation), text))\n",
        "    return punct_counter\n",
        "\n",
        "\n",
        "@custom_transformer\n",
        "def punctuation_count(text: pd.Series) -> pd.DataFrame:\n",
        "    return pd.DataFrame(text.apply(lambda row: punctuation_count_calc(row)))\n",
        "\n",
        "\n",
        "def qwords_count_calc(text: str) -> pd.DataFrame:\n",
        "    text = text.lower()\n",
        "    qword_counter = 0\n",
        "    for i, question in enumerate(QUESTION_WORDS):\n",
        "        qword_counter += len(re.findall('[{}]'.format(question), text))\n",
        "    return qword_counter\n",
        "\n",
        "\n",
        "@custom_transformer\n",
        "def qwords_count(text: pd.Series) -> pd.DataFrame:\n",
        "    return pd.DataFrame(text.apply(lambda row: qwords_count_calc(row)))\n",
        "\n",
        "\n",
        "@custom_transformer\n",
        "def number_count(text: pd.Series) -> pd.DataFrame:\n",
        "    return pd.DataFrame(text.apply(lambda row: len(re.findall(r'[0-9]', row))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZN_d-8F3-TY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "\n",
        "ohe = oh_encoder()\n",
        "\n",
        "preprocess = ColumnTransformer([\n",
        "    # ('qt_char_count', char_count, 'question_title'),\n",
        "    # ('qb_char_count', char_count, 'question_body'),\n",
        "    # ('a_char_count', char_count, 'answer'),\n",
        "    \n",
        "    # ('qt_word_count', word_count, 'question_title'),\n",
        "    # ('qb_word_count', word_count, 'question_body'),\n",
        "    # ('a_word_count', word_count, 'answer'),\n",
        "\n",
        "    # ('qt_unique_word_count', unique_word_count, 'question_title'),\n",
        "    # ('qb_unique_word_count', unique_word_count, 'question_body'),\n",
        "    # ('a_unique_word_count', unique_word_count, 'answer'),\n",
        "\n",
        "    # ('qt_sentence_count', sentence_count, 'question_title'),\n",
        "    # ('qb_sentence_count', sentence_count, 'question_body'),\n",
        "    # ('a_sentence_count', sentence_count, 'answer'),\n",
        "\n",
        "    # ('qt_stopword_ratio', stopword_ratio, 'question_title'),\n",
        "    # ('qb_stopword_ratio', stopword_ratio, 'question_body'),\n",
        "    # ('a_stopword_ratio', stopword_ratio, 'answer'),\n",
        "\n",
        "    # ('qt_uppercase_ratio', uppercase_ratio, 'question_title'),\n",
        "    # ('qb_uppercase_ratio', uppercase_ratio, 'question_body'),\n",
        "    # ('a_uppercase_ratio', uppercase_ratio, 'answer'),\n",
        "\n",
        "    # ('qt_punctuation_count', punctuation_count, 'question_title'),\n",
        "    # ('qb_punctuation_count', punctuation_count, 'question_body'),\n",
        "    # ('a_punctuation_count', punctuation_count, 'answer'),\n",
        "\n",
        "    # ('qt_qwords_count', qwords_count, 'question_title'),\n",
        "    # ('qb_qwords_count', qwords_count, 'question_body'),\n",
        "\n",
        "    # ('qt_number_count', number_count, 'question_title'),\n",
        "    # ('qb_number_count', number_count, 'question_body'),\n",
        "    # ('a_number_count', number_count, 'answer'),\n",
        "\n",
        "    ('host_ohe', ohe, 'host'),\n",
        "    ('category_ohe', ohe, 'category')\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCstX7nt35Ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_metadata_feat = preprocess.fit_transform(train_dset.drop(target_columns, axis=1))\n",
        "# test_metadata_feat = preprocess.transform(test_dset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft61tJY54Da7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "whitening_preprocess = Pipeline([\n",
        "                                ('Normalizer', MinMaxScaler(feature_range=(-1,1))),\n",
        "                                ('Standarization', StandardScaler()),\n",
        "                        ])\n",
        "\n",
        "train_metadata_feat = whitening_preprocess.fit_transform(train_metadata_feat)\n",
        "# test_metadata_feat = whitening_preprocess.transform(test_metadata_feat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D55Zeitb2eNx",
        "colab_type": "text"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF2dml-u2ghC",
        "colab_type": "code",
        "outputId": "0639cd4b-1b04-4e35-d42f-ad26c60053e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "from math import floor, ceil\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import transformers\n",
        "\n",
        "\n",
        "BERT_PATH = PATH+'bert/bert/'\n",
        "# https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_bert.py#L34\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "FIRST_SENT_MAX_LENGTH = 50\n",
        "SECOND_SENT_MIN_LENGTH = MAX_SEQUENCE_LENGTH - (FIRST_SENT_MAX_LENGTH + DUAL_SENT_SPECIAL_TOKENS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAmhTKgoeztZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### BERT preprocessing tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoFmB6Ez29Tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bert_dual_sentence_preprocess(first_sent, second_sent, tokenizer):\n",
        "    first_sent_tokens = tokenizer.tokenize(first_sent)\n",
        "    second_sent_tokens = tokenizer.tokenize(second_sent)\n",
        "\n",
        "    first_sent_ids = tokenizer.convert_tokens_to_ids(first_sent_tokens)\n",
        "    second_sent_ids = tokenizer.convert_tokens_to_ids(second_sent_tokens)\n",
        "\n",
        "    first_sent_mask = list(np.ones(len(first_sent_ids)).astype(int))\n",
        "    second_sent_mask = list(np.ones(len(second_sent_ids)).astype(int))\n",
        "\n",
        "    first_sent_segm = list(np.zeros(len(first_sent_ids)).astype(int))\n",
        "    second_sent_segm = list(np.ones(len(second_sent_ids)).astype(int))\n",
        "\n",
        "    if len(first_sent_ids) < FIRST_SENT_MAX_LENGTH:\n",
        "        second_sent_length = SECOND_SENT_MIN_LENGTH + (FIRST_SENT_MAX_LENGTH-len(first_sent_ids))\n",
        "    else:\n",
        "        second_sent_length = SECOND_SENT_MIN_LENGTH\n",
        "\n",
        "    input_ids = [tokenizer.cls_token_id] + first_sent_ids[0:FIRST_SENT_MAX_LENGTH] + \\\n",
        "                [tokenizer.sep_token_id] + second_sent_ids[0:second_sent_length] + [tokenizer.sep_token_id]\n",
        "\n",
        "    input_masks = [1] + first_sent_mask[0:FIRST_SENT_MAX_LENGTH] + [1] + second_sent_mask[0:second_sent_length] + [1]\n",
        "    input_segments = [0] + first_sent_segm[0:FIRST_SENT_MAX_LENGTH] + [0] + second_sent_segm[0:second_sent_length] + [1]\n",
        "  \n",
        "    input_ids = input_ids + ([tokenizer.pad_token_id] * (MAX_SEQUENCE_LENGTH - len(input_ids)))\n",
        "    input_masks = input_masks + ([0] * (MAX_SEQUENCE_LENGTH - len(input_masks)))\n",
        "    input_segments = input_segments + ([0] * (MAX_SEQUENCE_LENGTH - len(input_segments)))\n",
        "    \n",
        "    return [input_ids, input_masks, input_segments]\n",
        "\n",
        "\n",
        "def compute_input_arays(df, columns, tokenizer, max_sequence_length):\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for _, instance in tqdm(df[columns].iterrows()):\n",
        "        first_sent, second_sent = instance[columns[0]], instance[columns[1]]\n",
        "        ids, masks, segments = bert_dual_sentence_preprocess(first_sent, second_sent, tokenizer)\n",
        "    \n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "\n",
        "    return [torch.tensor(input_ids, dtype=torch.long),\n",
        "            torch.tensor(input_masks, dtype=torch.long),\n",
        "            torch.tensor(input_segments, dtype=torch.long)]\n",
        "\n",
        "\n",
        "def compute_output_arrays(df, columns):\n",
        "    return torch.tensor(df[columns].values, dtype=torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEwfQDih7fA1",
        "colab_type": "text"
      },
      "source": [
        "#### Text preprocessing to BERT inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhHxPxtGdysL",
        "colab_type": "code",
        "outputId": "8bbd078d-c6f0-464c-d87e-99da1a44c880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181,
          "referenced_widgets": [
            "c616e1d5fd7c40389ef58cafc1348622",
            "c9141c1a3dce4800b5ea95d486a74d53",
            "aecba41b3f804a6ea7cd5c7eea1cda52",
            "bcddd7e172524e498a70ef8157e69bcc",
            "2e4d666095514409989e01e507e77179",
            "efa528b949aa4c328f2b26a25bc721bf",
            "4c1f8bfc501d4991a46a190de2121df9",
            "5666c61057cd4cbb990f48361b379d36",
            "0295b92afea341b99684612c1442832d",
            "bc4878fcfeb14746b246b11d734e01c6",
            "f0131405800b48a48429d237802fcf82",
            "a40b2f4b186b47d8986c64c35c2b5d6c",
            "54da01792b054316b4678b5bdb2c9b70",
            "6a39c5b3a4794dbb99cab7febb317257",
            "81de10cbe01544a285ae86c9a38860e7",
            "ee3b5432cddd4d84828c1cfd0b2eed33",
            "5939841ce19a4f5cb20bb947e2534ddd",
            "64c63c4620e2480a927a5de4b25beaee",
            "856eef928cf34ae2b0034685ef01b857",
            "20e6350cb8224c6dbb43b68ec31a959a",
            "50a6dbaef49e463ca91f3e17699674d3",
            "c1e934bad4b749b2b01ea654bfeb94ce",
            "47c835df1bd94342b2845cb805bf6c04",
            "bc8b6db94abe424cb7f309d9aca6f194"
          ]
        }
      },
      "source": [
        "import gc \n",
        "\n",
        "FOLDS = 10\n",
        "gkf = GroupKFold(n_splits=FOLDS).split(X=train_dset.question_body, groups=train_dset.question_body)\n",
        "\n",
        "train_targets = compute_output_arrays(train_dset, target_columns)\n",
        "\n",
        "train_body_inputs = compute_input_arays(train_dset, ['question_title', 'question_body'], tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "train_answer_inputs = compute_input_arays(train_dset, ['question_title', 'answer'], tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "metadata_feat_len = len(train_metadata_feat[0])\n",
        "train_inputs = train_body_inputs + train_answer_inputs + list(torch.tensor([train_metadata_feat], dtype=torch.float16))\n",
        "\n",
        "del tokenizer, train_dset, test_dset, train_body_inputs, train_answer_inputs\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c616e1d5fd7c40389ef58cafc1348622",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0295b92afea341b99684612c1442832d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5939841ce19a4f5cb20bb947e2534ddd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZpdauezdgJP",
        "colab_type": "text"
      },
      "source": [
        "### BERT Dataset tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kNcaYaA0vxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bert_dataset(inputs, idx=[], targets=torch.tensor([])):\n",
        "    if len(targets) > 0 and len(idx) > 0:\n",
        "        return data.TensorDataset(inputs[0][idx], # body_input_ids\n",
        "                                  inputs[1][idx], # body_input_masks\n",
        "                                  inputs[2][idx], # body_input_segments\n",
        "                                  inputs[3][idx], # answer_input_ids\n",
        "                                  inputs[4][idx], # answer_input_masks\n",
        "                                  inputs[5][idx], # answer_input_segments\n",
        "                                  inputs[6][idx], # metadata_features\n",
        "                                  targets[idx] #targets\n",
        "                                  )\n",
        "    elif len(targets) > 0:\n",
        "        return data.TensorDataset(inputs[0][:],\n",
        "                                  inputs[1][:],\n",
        "                                  inputs[2][:],\n",
        "                                  inputs[3][:],\n",
        "                                  inputs[4][:],\n",
        "                                  inputs[5][:],\n",
        "                                  inputs[6][:],\n",
        "                                  targets[:]\n",
        "                                  )\n",
        "    elif len(idx) > 0:\n",
        "        return data.TensorDataset(inputs[0][idx],\n",
        "                                  inputs[1][idx],\n",
        "                                  inputs[2][idx],\n",
        "                                  inputs[3][idx],\n",
        "                                  inputs[4][idx],\n",
        "                                  inputs[5][idx],\n",
        "                                  inputs[6][idx]\n",
        "                                  )\n",
        "    else: \n",
        "        return data.TensorDataset(inputs[0][:],\n",
        "                                  inputs[1][:],\n",
        "                                  inputs[2][:],\n",
        "                                  inputs[3][:],\n",
        "                                  inputs[4][:],\n",
        "                                  inputs[5][:],\n",
        "                                  inputs[6][:]\n",
        "                                  )\n",
        "\n",
        "\n",
        "def compute_spearmanr(preds, trues):\n",
        "    rhos = []\n",
        "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
        "        rhos.append(\n",
        "            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n",
        "    return np.mean(rhos), rhos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgnoU0y875oq",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkvTOBTmN6eM",
        "colab_type": "text"
      },
      "source": [
        "### Custom BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-kH63j-NwfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import multiprocessing, glob\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "from torch import nn\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader, Dataset,RandomSampler, SequentialSampler\n",
        "from transformers import (\n",
        "    BertTokenizer, BertModel, BertConfig,\n",
        "    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n",
        "    get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "    )\n",
        "from transformers.modeling_bert import BertPreTrainedModel\n",
        "from utils.sodeep import sodeep\n",
        "\n",
        "\n",
        "class CustomBERTBaseUncased(nn.Module):\n",
        "    def __init__(self, bert_model_path: str, bert_cfg_path: str, dropout: float, output_len: int, metadata_feat_len: int):\n",
        "        super(CustomBERTBaseUncased, self).__init__()\n",
        "        \n",
        "        hidden_bert_size = 768*4*2 # bert output * heads * models\n",
        "        bert_metadata_size = hidden_bert_size + metadata_feat_len\n",
        "        hidden_layer1_size = int(hidden_bert_size/4) + int(metadata_feat_len/2)\n",
        "        hidden_layer2_size = int(hidden_layer1_size/4)\n",
        "        \n",
        "        self.body_bert = transformers.BertModel.from_pretrained(bert_model_path, config=bert_cfg_path)\n",
        "        self.answer_bert = transformers.BertModel.from_pretrained(bert_model_path, config=bert_cfg_path)\n",
        "        \n",
        "        self.multi_sample_dropout = nn.Dropout(0.5) # SpatialDropout\n",
        "        \n",
        "        self.metadata_layer = nn.Linear(metadata_feat_len, metadata_feat_len)\n",
        "        \n",
        "        self.mlp_dense_1 = nn.Linear(bert_metadata_size, hidden_layer1_size)\n",
        "        self.mlp_hidden_1 = nn.SELU()\n",
        "        \n",
        "        self.mlp_dense_2 = nn.Linear(hidden_layer1_size, hidden_layer2_size)\n",
        "        self.mlp_hidden_2 = nn.SELU()\n",
        "        \n",
        "        self.mlp_drop = nn.Dropout(dropout)\n",
        "        self.mlp_linear = nn.Linear(hidden_layer2_size, output_len)\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        body_ids = data[0].to(device, dtype=torch.long)\n",
        "        body_masks = data[1].to(device, dtype=torch.long)\n",
        "        body_segments = data[2].to(device, dtype=torch.long)\n",
        "        answer_ids = data[3].to(device, dtype=torch.long)\n",
        "        answer_masks = data[4].to(device, dtype=torch.long)\n",
        "        answer_segments = data[5].to(device, dtype=torch.long)\n",
        "        metadata_inputs = data[6].to(device, dtype=torch.float16)\n",
        "\n",
        "        body_bert_output = self.body_bert(body_ids, attention_mask=body_masks, token_type_ids=body_segments)\n",
        "        answer_bert_output = self.answer_bert(answer_ids, attention_mask=answer_masks, token_type_ids=answer_segments)\n",
        "        \n",
        "        body_bert_output = body_bert_output[2][-4:]\n",
        "        answer_bert_output = answer_bert_output[2][-4:]\n",
        "        \n",
        "        body_bert_heads = torch.cat(([self.multi_sample_dropout(layer) for layer in body_bert_output]), 2)\n",
        "        answer_bert_heads = torch.cat(([self.multi_sample_dropout(layer) for layer in answer_bert_output]), 2)\n",
        "        concat_bert_heads = torch.cat((body_bert_heads, answer_bert_heads), 2)\n",
        "        \n",
        "        bert_layer = torch.mean(concat_bert_heads, 1)\n",
        "\n",
        "        metadata_layer = self.metadata_layer(metadata_inputs)\n",
        "        concat_bert_metadata = torch.cat((bert_layer, metadata_layer), 1)\n",
        "\n",
        "        mlp_layer_1 = self.mlp_dense_1(concat_bert_metadata)\n",
        "        mlp_hidden_1 = self.mlp_hidden_1(mlp_layer_1)\n",
        "\n",
        "        mlp_layer_2 = self.mlp_dense_2(mlp_hidden_1)\n",
        "        mlp_hidden_2 = self.mlp_hidden_2(mlp_layer_2)\n",
        "\n",
        "        mlp_drop = self.mlp_drop(mlp_hidden_2)\n",
        "        \n",
        "        mlp_linear = self.mlp_linear(mlp_hidden_2)\n",
        "        return mlp_linear\n",
        "\n",
        "\n",
        "def loss_fn(criterion, outputs, targets):\n",
        "    most_diff_targets = ['question_not_really_a_question', 'question_type_spelling'] \n",
        "    diff_targets = ['answer_plausible', 'answer_well_written', 'answer_relevance', 'question_type_consequence', \n",
        "                    'answer_helpful', 'question_expect_short_answer', 'answer_satisfaction']\n",
        "    mild_diff_targets = ['answer_type_procedure', 'question_fact_seeking', 'question_interestingness_others', \n",
        "                         'question_type_definition', 'question_type_compare', 'question_type_procedure', \n",
        "                         'question_conversational', 'question_asker_intent_understanding', \n",
        "                         'question_has_commonly_accepted_answer']\n",
        "\n",
        "    weights = np.ones(len(target_columns))\n",
        "    for unbalanced_col in most_diff_targets:\n",
        "        weights[target_columns.index(unbalanced_col)] = 2\n",
        "    for unbalanced_col in diff_targets:\n",
        "        weights[target_columns.index(unbalanced_col)] = 1.25\n",
        "    for unbalanced_col in mild_diff_targets:\n",
        "        weights[target_columns.index(unbalanced_col)] = 1.1\n",
        "\n",
        "    loss = [criterion(outputs[i], targets[i]) for i in range(len(outputs))]\n",
        "    weighted_loss = [loss[i]*weights[i] for i in range(len(outputs))]\n",
        "    return sum(weighted_loss)\n",
        "\n",
        "\n",
        "def train_loop_fn(data_loader, model, criterion, optimizer, device, scheduler=None):\n",
        "    model.train()\n",
        "    total_batches = len(train_loader)\n",
        "    train_progress = tqdm(enumerate(train_loader), total=total_batches)\n",
        "    for batch_idx, data in train_progress:\n",
        "        if len(data[0]) > 1:    # Discard last batch if size 1 to avoid breaking BatchNorm\n",
        "            targets = data[10].to(device, dtype=torch.float32)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            \n",
        "            loss = loss_fn(criterion, outputs, targets)\n",
        "            \n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        train_progress.set_description(f'Batch: {batch_idx} - Loss: {loss}')\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def eval_loop_fn(data_loader, model, criterion, device):\n",
        "    model.eval()\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    with torch.no_grad():\n",
        "        eval_progress = tqdm(enumerate(data_loader), total=len(data_loader))\n",
        "        for batch_idx, data in eval_progress:\n",
        "            targets = data[7].to(device, dtype=torch.float32)\n",
        "            outputs = model(data)\n",
        "            loss = loss_fn(criterion, outputs, targets)\n",
        "            fin_targets.append(targets.cpu().detach().numpy())\n",
        "            fin_outputs.append(outputs.cpu().detach().numpy())\n",
        "            \n",
        "            eval_progress.set_description(f'Predicting Eval Batch: {batch_idx}')\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "        \n",
        "    return np.vstack(fin_outputs), np.vstack(fin_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-cFgAnz8ECq",
        "colab_type": "text"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWXHD1fXJ-si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "EPOCHS = 4\n",
        "\n",
        "\n",
        "def get_previous_train_info(fold):\n",
        "    remaining_epochs = EPOCHS\n",
        "    weights_path = None\n",
        "    checkpoint_path = None\n",
        "    epochs_run = sum(['.pt' in filename for filename in os.listdir(PATH+f'model/{MODEL_NAME}/fold{fold}/')])\n",
        "    if epochs_run > 0: \n",
        "        last_epoch = epochs_run - 1\n",
        "        remaining_epochs = EPOCHS - epochs_run\n",
        "        weights_path = PATH+f'model/{MODEL_NAME}/fold{fold}/{MODEL_NAME}-{fold}-{last_epoch}.pt'\n",
        "        checkpoint_path = PATH+f'model/{MODEL_NAME}/fold{fold}/{MODEL_NAME}-{fold}.chkpt'\n",
        "    return weights_path, checkpoint_path, remaining_epochs, epochs_run\n",
        "\n",
        "\n",
        "def save_model(model, optimizer, fold, epoch, previous_epoch=0):\n",
        "    model = {'model': model.state_dict()}\n",
        "    checkpoint = {\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'amp': amp.state_dict()\n",
        "    }\n",
        "    real_epoch = epoch + previous_epoch\n",
        "    torch.save(model, PATH+f'model/{MODEL_NAME}/fold{fold}/{MODEL_NAME}-{fold}-{real_epoch}.pt')\n",
        "    torch.save(checkpoint, PATH+f'model/{MODEL_NAME}/fold{fold}/{MODEL_NAME}-{fold}.chkpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQAa_GGNfEw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "ACCUM_STEPS = 1\n",
        "DEVICE = 'cuda'\n",
        "LR = 3e-5\n",
        "\n",
        "max_folds = FOLDS -1\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
        "    print(f\"Current Fold: {fold}\")\n",
        "\n",
        "    if fold == max_folds:   # Full train\n",
        "        train_set = bert_dataset(inputs=train_inputs, targets=train_targets)        \n",
        "        valid_set = bert_dataset(inputs=train_inputs, targets=train_targets)\n",
        "    elif 0 <= fold < max_folds:\n",
        "        train_set = bert_dataset(inputs=train_inputs, idx=train_idx, targets=train_targets)        \n",
        "        valid_set = bert_dataset(inputs=train_inputs, idx=valid_idx, targets=train_targets)\n",
        "\n",
        "    if fold <= max_folds:\n",
        "        \n",
        "        weights_path, checkpoint_path, total_epochs, previous_epochs_run = get_previous_train_info(fold)\n",
        "        if total_epochs > 0:\n",
        "            train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "            valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "            set_seeds(SEED*fold)\n",
        "            device = DEVICE\n",
        "            lr = LR\n",
        "            epoch_train_steps = int(len(train_idx) / BATCH_SIZE)\n",
        "            num_train_steps = epoch_train_steps * EPOCHS\n",
        "            \n",
        "            model = CustomBERTBaseUncased(bert_model_path= BERT_PATH+'bert-base-uncased-pytorch_model.bin',\n",
        "                                        bert_cfg_path= BERT_PATH+'bert-base-uncased-config.json',\n",
        "                                        dropout= 0.2,\n",
        "                                        output_len= len(target_columns),\n",
        "                                        metadata_feat_len=metadata_feat_len)\n",
        "            model.zero_grad();\n",
        "            model.to(device);\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            param_optimizer = list(model.named_parameters())\n",
        "            no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "            optimizer_grouped_parameters = [\n",
        "                                            {'params': [p for n, p in param_optimizer \n",
        "                                                        if not any(nd in n for nd in no_decay)],\n",
        "                                            'weight_decay': 0.8},\n",
        "                                            {'params': [p for n, p in param_optimizer \n",
        "                                                        if any(nd in n for nd in no_decay)], \n",
        "                                            'weight_decay': 0.0}\n",
        "                                            ]\n",
        "            optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, eps=4e-5)\n",
        "            \n",
        "            model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\", \n",
        "                                              keep_batchnorm_fp32=True, loss_scale=\"dynamic\") \n",
        "            \n",
        "            if weights_path:\n",
        "                print(f'Training remaining {total_epochs} epochs with checkpoint {weights_path}')\n",
        "\n",
        "                saved_model = torch.load(weights_path, map_location=lambda storage, loc: storage)\n",
        "                model.load_state_dict(saved_model['model'])\n",
        "                checkpoint = torch.load(checkpoint_path, \n",
        "                                        map_location=lambda storage, loc: storage)\n",
        "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "                amp.load_state_dict(checkpoint['amp'])\n",
        "                del saved_model, checkpoint\n",
        "            \n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, \n",
        "                                                        num_training_steps=num_train_steps)\n",
        "            \n",
        "            for epoch in range(total_epochs):\n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "                criterion = nn.BCEWithLogitsLoss()\n",
        "                criterion.to(device)\n",
        "\n",
        "                train_loop_fn(train_loader, model, criterion, optimizer, device, scheduler)\n",
        "                \n",
        "                save_model(model, optimizer, fold, epoch, previous_epoch=previous_epochs_run)\n",
        "\n",
        "                outputs, targets = eval_loop_fn(valid_loader, model, criterion, device)\n",
        "\n",
        "                spear, rho_cols = compute_spearmanr(outputs, targets)\n",
        "                print(f'epoch = {epoch}, spearman = {spear}')\n",
        "                rho_print = [print(target_columns[i] + \" rho: \" + str(rho_cols[i]) ) for \n",
        "                             i in range(0, len(target_columns))]\n",
        "                \n",
        "            del train_set, train_loader, valid_set, valid_loader, model, optimizer, scheduler\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmQlW-w8z17k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_spearmanr_withnan(preds, trues):\n",
        "    rhos = []\n",
        "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
        "        rhos.append(\n",
        "            spearmanr(col_trues, col_pred).correlation)\n",
        "    return np.mean(rhos), rhos\n",
        "\n",
        "def score_postprocess(outputs, targets, target_columns, p):\n",
        "    relu_cols_idx = []\n",
        "    for unbalanced_col in ['question_not_really_a_question', 'question_type_consequence', 'question_type_spelling']:\n",
        "        relu_cols_idx.append(target_columns.index(unbalanced_col))\n",
        "    \n",
        "    relu_outputs = outputs.copy()\n",
        "    for idx in relu_cols_idx:\n",
        "        if relu_outputs[:,idx].max() < 0:\n",
        "            center_k = abs(np.percentile(relu_outputs[:,idx], p))\n",
        "        else:\n",
        "            center_k = 0\n",
        "        relu_outputs[:,idx] = np.array(torch.functional.F.relu(torch.Tensor(relu_outputs[:,idx] + center_k)))\n",
        "    \n",
        "    norm_out = (relu_outputs - relu_outputs.min(axis=0)) / (relu_outputs.max(axis=0) - relu_outputs.min(axis=0))\n",
        "    spear, rho_cols = compute_spearmanr_withnan(norm_out, targets)\n",
        "    print(f'epoch = {epoch}, spearman = {spear}')\n",
        "    rho_print = [print(target_columns[i] + \" rho: \" + str(rho_cols[i]) ) for i in relu_cols_idx]\n",
        "    return relu_outputs, norm_out"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
